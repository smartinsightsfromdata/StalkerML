{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sqlite3\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('SpiegelCrawler')\n",
    "logger.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(path.join('logs', 'spiegel_crawler.log'))\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_file = path.join('Data', 'archive_spiegel.sqlite')\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_url(url):\n",
    "    if url.startswith('/'):\n",
    "        return ''.join(['http://www.spiegel.de', url])\n",
    "    else:\n",
    "        return url\n",
    "\n",
    "def extract_category(s):\n",
    "    return s.strip().replace('(', '').replace(')', '').split(',')\n",
    "\n",
    "def generate_date(date, time):\n",
    "    hours, minutes = time.split(':')\n",
    "    day, month, year = date.split('.')\n",
    "    return dt.datetime(int(year), int(month), int(day), int(hours), int(minutes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for date in dates:\n",
    "    data = []\n",
    "    url = 'http://www.spiegel.de/nachrichtenarchiv/artikel-{}.html'.format(date)\n",
    "    try:\n",
    "        day = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(day.text)\n",
    "        articles = soup.find('div', class_='column-wide')\n",
    "        for article in articles.find_all('li'):\n",
    "            article_url = generate_url(article.a['href'])\n",
    "            title = article.a['title']\n",
    "            category, t = extract_category(article.find('span', class_='headline-date').contents[0])\n",
    "            dtime = generate_date(date, t)\n",
    "            try:\n",
    "                html = requests.get(article_url, stream=True, timeout=10).content\n",
    "                data.append( (title, dtime, category, article_url, html) )\n",
    "            except RequestException as error:\n",
    "                logger.error('ARTICLE FAIL: %s : %s, %s', error, article_url, date)\n",
    "        cursor.executemany(sql_insert, data)\n",
    "        conn.commit()\n",
    "    except RequestException as error:\n",
    "        logger.error('DAY FAIL: %s : %s, %s', error, url, date)\n",
    "    except AttributeError as error:\n",
    "        logger.error('DAY FAIL: %s : %s, %s', error, url, date)\n",
    "    else:\n",
    "        logger.info('Successfully crawled articles from: %s', date)\n",
    "    time.sleep(2)\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
