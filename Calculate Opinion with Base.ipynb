{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Political Opinion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "from os import path\n",
    "from corputil import FileCorpus\n",
    "from corputil.utils import load_stopwords\n",
    "from gensim.models.word2vec import LineSentence, Word2Vec\n",
    "\n",
    "stopwords = load_stopwords(path.join('data', 'german.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Base Model\n",
    "\n",
    "Calculate the base model (from german wiki), that is later used as a base for training the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentences = LineSentence(path.join('data', 'Archive', 'Corpus_Wiki.txt'))\n",
    "# base = Word2Vec(sentences, workers=4, size=300, window=10, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model to disk. Don't finalize the model because we need to train it with new data later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base.save(path.join('models', 'word2vec', 'base.w2v'))\n",
    "# base = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for Die Linke.\n",
    "Model is finalized to save RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = path.join('data', 'Politics', 'Corpus_Linke_Fraktion.txt')\n",
    "corpus = list(FileCorpus(file).sentences_token(stopwords=stopwords))\n",
    "\n",
    "linke = Word2Vec.load(path.join('models', 'word2vec', 'base.w2v'))\n",
    "linke.train(corpus, total_examples=len(corpus))\n",
    "# linke.init_sims(replace=True) Doesn't work for now!\n",
    "linke.save(path.join('models', 'word2vec', 'Linke.w2v'))\n",
    "linke = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for SPD.\n",
    "Model is finalized to save RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = path.join('data', 'Politics', 'Corpus_SPD_Fraktion.txt')\n",
    "corpus = list(FileCorpus(file).sentences_token(stopwords=stopwords))\n",
    "\n",
    "spd = Word2Vec.load(path.join('models', 'word2vec', 'base.w2v'))\n",
    "spd.train(corpus, total_examples=len(corpus))\n",
    "# spd.init_sims(replace=True) Doesn't work for now!\n",
    "spd.save(path.join('models', 'word2vec', 'SPD.w2v'))\n",
    "spd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for Die Gr端nen.\n",
    "Model is finalized to save RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = path.join('data', 'Politics', 'Corpus_Gr端ne_Fraktion.txt')\n",
    "corpus = list(FileCorpus(file).sentences_token(stopwords=stopwords))\n",
    "\n",
    "gruene = Word2Vec.load(path.join('models', 'word2vec', 'base.w2v'))\n",
    "gruene.train(corpus, total_examples=len(corpus))\n",
    "# gruene.init_sims(replace=True) Doesn't work for now!\n",
    "gruene.save(path.join('models', 'word2vec', 'Gr端ne.w2v'))\n",
    "gruene = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for FDP. Model is finalized to save RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = path.join('data', 'Politics', 'Corpus_FDP_Fraktion.txt')\n",
    "corpus = list(FileCorpus(file).sentences_token(stopwords=stopwords))\n",
    "\n",
    "fdp = Word2Vec.load(path.join('models', 'word2vec', 'base.w2v'))\n",
    "fdp.train(corpus, total_examples=len(corpus))\n",
    "# fdp.init_sims(replace=True) Doesn't work for now!\n",
    "fdp.save(path.join('models', 'word2vec', 'FDP.w2v'))\n",
    "fdp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for CDU. Model is finalized to save RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = path.join('data', 'Politics', 'Corpus_CDU_Fraktion.txt')\n",
    "corpus = list(FileCorpus(file).sentences_token(stopwords=stopwords))\n",
    "\n",
    "cdu = Word2Vec.load(path.join('models', 'word2vec', 'base.w2v'))\n",
    "cdu.train(corpus, total_examples=len(corpus))\n",
    "# cdu.init_sims(replace=True) Doesn't work for now!\n",
    "cdu.save(path.join('models', 'word2vec', 'CDU.w2v'))\n",
    "cdu = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for NPD. Model is finalized to save RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file1 = path.join('data', 'Politics', 'Corpus_NPD_MV.txt')\n",
    "file2 = path.join('data', 'Politics', 'Corpus_NPD_Sachsen.txt')\n",
    "corpus = list(FileCorpus(file1, file2).sentences_token(stopwords=stopwords))\n",
    "\n",
    "npd = Word2Vec.load(path.join('models', 'word2vec', 'base.w2v'))\n",
    "npd.train(corpus, total_examples=len(corpus))\n",
    "# npd.init_sims(replace=True) Doesn't work for now!\n",
    "npd.save(path.join('models', 'word2vec', 'NPD.w2v'))\n",
    "npd = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models and documents into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [path.join('models', 'word2vec', 'SPD.w2v'),\n",
    "          path.join('models', 'word2vec', 'Linke.w2v'),\n",
    "          path.join('models', 'word2vec', 'Gr端ne.w2v'),\n",
    "          path.join('models', 'word2vec', 'FDP.w2v'),\n",
    "          path.join('models', 'word2vec', 'CDU.w2v'),\n",
    "          path.join('models', 'word2vec', 'NPD.w2v')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_score(doc, mod):\n",
    "    model = Word2Vec.load(mod)\n",
    "    score = model.score(doc, len(doc))\n",
    "    return score\n",
    "\n",
    "def calc_probability(docs, mods):\n",
    "    sentlist = [s for d in docs for s in d]\n",
    "    llhd = np.array( [ calc_score(sentlist, m) for m in mods ] )\n",
    "    lhd = np.exp(llhd - llhd.max(axis=0))\n",
    "    prob = pd.DataFrame( (lhd/lhd.sum(axis=0)).transpose() )\n",
    "    prob[\"doc\"] = [i for i,d in enumerate(docs) for s in d]\n",
    "    prob = prob.groupby(\"doc\").mean()\n",
    "    prob = prob.sum()\n",
    "    return prob/prob.sum()\n",
    "\n",
    "# KW44 = path.join('data', 'CurrentNews', '2015KW44_Spiegel.txt')\n",
    "# KW45 = path.join('data', 'CurrentNews', '2015KW45_Spiegel.txt')\n",
    "# KW46 = path.join('data', 'CurrentNews', '2015KW46_Spiegel.txt')\n",
    "# KW47 = path.join('data', 'CurrentNews', '2015KW47_Spiegel.txt')\n",
    "# KW48 = path.join('data', 'CurrentNews', '2015KW48_Spiegel.txt')\n",
    "# KW49 = path.join('data', 'CurrentNews', '2015KW49_Spiegel.txt')\n",
    "# KW50 = path.join('data', 'CurrentNews', '2015KW50_Spiegel.txt')\n",
    "# corpus = list(FileCorpus(KW44, KW45, KW46, KW47, KW48, KW49, KW50).doc_sentences_token(stopwords=stopwords))\n",
    "\n",
    "file = path.join('data', 'CurrentNews', '2015KW44_Spiegel.txt')\n",
    "corpus = list(FileCorpus(file).doc_sentences_token())\n",
    "print(calc_probability(corpus, models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained without stopwords, query without stopwords\n",
    "\n",
    "Spiegel\n",
    "\n",
    "Welt\n",
    "\n",
    "Handelsblatt\n",
    "0    0.091655\n",
    "1    0.265744\n",
    "2    0.192597\n",
    "3    0.166589\n",
    "4    0.197317\n",
    "5    0.086098\n",
    "\n",
    "Junge Freiheit\n",
    "\n",
    "Junge Welt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained without stopwords, query with stopwords\n",
    "\n",
    "Spiegel\n",
    "0    0.093173\n",
    "1    0.188099\n",
    "2    0.164464\n",
    "3    0.138946\n",
    "4    0.159299\n",
    "5    0.256019\n",
    "\n",
    "Welt\n",
    "0    0.087746\n",
    "1    0.248905\n",
    "2    0.142936\n",
    "3    0.194455\n",
    "4    0.130204\n",
    "5    0.195753\n",
    "\n",
    "Handelsblatt\n",
    "0    0.083737\n",
    "1    0.255106\n",
    "2    0.185700\n",
    "3    0.163751\n",
    "4    0.148524\n",
    "5    0.163182\n",
    "\n",
    "Junge Freiheit\n",
    "0    0.091618\n",
    "1    0.213073\n",
    "2    0.110979\n",
    "3    0.201321\n",
    "4    0.092264\n",
    "5    0.290745\n",
    "\n",
    "Junge Welt\n",
    "0    0.045699\n",
    "1    0.215526\n",
    "2    0.091375\n",
    "3    0.421882\n",
    "4    0.076452\n",
    "5    0.149066"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained with stopwords, query without stopwords\n",
    "\n",
    "Spiegel\n",
    "\n",
    "Welt\n",
    "\n",
    "Handelsblatt\n",
    "\n",
    "Junge Freiheit\n",
    "\n",
    "Junge Welt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained with stopwords, query with stopwords\n",
    "\n",
    "Spiegel\n",
    "\n",
    "Welt\n",
    "\n",
    "Handelsblatt\n",
    "\n",
    "Junge Freiheit\n",
    "\n",
    "Junge Welt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
