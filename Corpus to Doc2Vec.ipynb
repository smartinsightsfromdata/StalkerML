{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, UnicodeDammit\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from readability.readability import Document\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('german'))\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/german.pickle')\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text(html):\n",
    "    html = UnicodeDammit(html, is_html=True)\n",
    "    article = Document(html.unicode_markup).summary()\n",
    "    article = BeautifulSoup(article).get_text()\n",
    "    return article\n",
    "\n",
    "def sentence_to_words(sentence):\n",
    "    letters_only = re.sub(r'[^a-zäöüA-ZÖÄÜß]', ' ', sentence)\n",
    "    words = letters_only.lower().split()\n",
    "    words = [w for w in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def document_to_sentences(document):\n",
    "    raw_sentences = tokenizer.tokenize(document.strip())\n",
    "    sentences = []\n",
    "    for sentence in raw_sentences:\n",
    "        sentences.append(sentence_to_words(sentence))\n",
    "    return sentences\n",
    "\n",
    "class StreamCorpus(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    def __iter__(self):\n",
    "        for document in open(self.filename, encoding='UTF-8'):\n",
    "            sentences = document_to_sentences(document)\n",
    "            for sentence in sentences:\n",
    "                yield sentence_to_words(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# tfidf = vectorizer.fit_transform(sentences)\n",
    "# idf = vectorizer.idf_\n",
    "# print(dict(zip(vectorizer.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
