{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "from corputil import FileCorpus\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_data(files, label):\n",
    "    sentences = [LabeledSentence(sentence, ['{}_{}'.format(label, i)]) \n",
    "                     for i, sentence in enumerate([s for doc in FileCorpus(files).doc_sentences_token() \n",
    "                                                     for s in doc])]\n",
    "    count = len(sentences)\n",
    "    return sentences, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spd = [\n",
    "    path.join('data', 'Politics', 'SPD_EU.txt'),\n",
    "    path.join('data', 'Politics', 'SPD_Fraktion.txt'),\n",
    "    path.join('data', 'Politics', 'SPD_Vorwärts_Inland.txt'),\n",
    "    path.join('data', 'Politics', 'SPD_Vorwärts_International.txt'),\n",
    "    path.join('data', 'Politics', 'SPD_Vorwärts_Parteileben.txt')\n",
    "]\n",
    "\n",
    "linke = [\n",
    "    path.join('data', 'Politics', 'Linke.txt'),\n",
    "    path.join('data', 'Politics', 'Linke_PR.txt'),\n",
    "    path.join('data', 'Politics', 'Linke_Fraktion.txt')\n",
    "]\n",
    "\n",
    "gruene = [\n",
    "    path.join('data', 'Politics', 'Grüne.txt'),\n",
    "    path.join('data', 'Politics', 'Grüne_Fraktion.txt')\n",
    "]\n",
    "\n",
    "fdp = [\n",
    "    path.join('data', 'Politics', 'FDP.txt'),\n",
    "    path.join('data', 'Politics', 'FDP_Fraktion.txt')\n",
    "]\n",
    "\n",
    "cdu = [\n",
    "    path.join('data', 'Politics', 'CDU.txt'),\n",
    "    path.join('data', 'Politics', 'CDU_Fraktion.txt')\n",
    "]\n",
    "\n",
    "npd = [\n",
    "    path.join('data', 'Politics', 'NPD_MV.txt'),\n",
    "    path.join('data', 'Politics', 'NPD_Sachsen.txt'),\n",
    "    path.join('data', 'Politics', 'NPD_Jung.txt')\n",
    "]\n",
    "\n",
    "files = [spd, linke, gruene, fdp, cdu, npd]\n",
    "tags = ['SPD', 'LINKE', 'GRUENE', 'FDP', 'CDU', 'NPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "counts = dict()\n",
    "for file, tag in zip(files, tags):\n",
    "    s, count = label_data(file, tag)\n",
    "    sentences += s\n",
    "    counts[tag] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=200, sample=1e-4, negative=5, workers=4)\n",
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    shuffle(sentences)\n",
    "    model.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'SPD': 0,\n",
    "    'LINKE': 1,\n",
    "    'GRUENE': 2,\n",
    "    'FDP': 3,\n",
    "    'CDU': 4,\n",
    "    'NPD': 5\n",
    "}\n",
    "\n",
    "index = 0\n",
    "train_arrays = np.zeros((len(sentences), 200))\n",
    "train_labels = np.zeros(len(sentences))\n",
    "for tag in tags:\n",
    "    for i in range(counts[tag]):\n",
    "        label = '{}_{}'.format(tag, i)\n",
    "        train_arrays[index + i] = model.docvecs[label]\n",
    "        train_labels[index + i] = mapping[tag]\n",
    "    index += counts[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb = [(vec, lab) for vec, lab in zip(train_arrays, train_labels)]\n",
    "shuffle(comb)\n",
    "train_arrays = [vec for vec, lab in comb[:800000]]\n",
    "train_labels = [lab for vec, lab in comb[:800000]]\n",
    "test_arrays = [vec for vec, lab in comb[800000:]]\n",
    "test_labels = [lab for vec, lab in comb[800000:]]\n",
    "comb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = LinearSVC()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier.score(train_arrays, train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
