{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from requests import RequestException\n",
    "from os import path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from html2corpus import HTML2Corpus\n",
    "from html2corpus.extractors import ReadabilityExtractor, ParagraphExtractor\n",
    "\n",
    "def check(link, blackwords):\n",
    "    return all([blackword not in link for blackword in blackwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling german political party sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Linke\n",
    "\n",
    "News from the website of the party: http://www.die-linke.de/nc/die-linke/nachrichten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.die-linke.de'\n",
    "keyword = 'artikel'\n",
    "\n",
    "site = 'http://www.die-linke.de/nc/die-linke/nachrichten'\n",
    "pages = ['{}/browse/{}'.format(site, i) for i in range(1, 99)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_DieLinke.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press releases from the website of the party: http://www.die-linke.de/nc/presse/presseerklaerungen/presseerklaerungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.die-linke.de'\n",
    "keyword = 'artikel'\n",
    "site = 'http://www.die-linke.de/nc/presse/presseerklaerungen/presseerklaerungen'\n",
    "pages = ['{}/browse/{}'.format(site, i) for i in range(1, 272)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_DieLinke_PR.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press releases from the faction: http://www.linksfraktion.de/pressemitteilungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.linksfraktion.de'\n",
    "keyword = 'pressemitteilungen'\n",
    "site = 'http://www.linksfraktion.de/pressemitteilungen'\n",
    "pages = ['{}/?s={}'.format(site, i) for i in range(1, 1384)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if link.get('href') and keyword in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_DieLinke_Fraktion.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPD\n",
    "\n",
    "Press releases from the website of the faction: http://www.spdfraktion.de/presse/pressemitteilungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.spdfraktion.de'\n",
    "keyword = 'presse/pressemitteilungen/'\n",
    "blackword = 'feed'\n",
    "site = 'http://www.spdfraktion.de/presse/pressemitteilungen'\n",
    "pages = ['{}?page={}'.format(site, i) for i in range(1, 733)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href'] and blackword not in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_SPD_Fraktion.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grüne\n",
    "\n",
    "Press releases from the website of the faction: http://www.gruene-bundestag.de/presse_ID_2000127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.gruene-bundestag.de'\n",
    "keyword = 'presse/pressemitteilungen/'\n",
    "blackword = 'feed'\n",
    "site = 'http://www.gruene-bundestag.de/presse_ID_2000127'\n",
    "pages = ['{}/pb_id/100/seite/{}'.format(site, i) for i in range(2, 1322)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href'] and blackword not in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_Grüne_Fraktion.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDP\n",
    "\n",
    "News from the website of the party: http://www.fdp.de/pressemitteilungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.fdp.de'\n",
    "keyword = '/content/'\n",
    "blackwords = set(['datenschutz', 'impressum'])\n",
    "site = 'http://www.fdp.de/pressemitteilungen'\n",
    "pages = ['{}?page={}'.format(site, i) for i in range(1, 97)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href'] and check(link['href'], blackwords)])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ParagraphExtractor(min_len=100)).save(path.join('data', 'Corpus_FDP.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PR from the faction: http://www.liberale.de/page/pressemitteilungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.liberale.de'\n",
    "keyword = '/content/'\n",
    "blackwords = set(['datenschutz', 'impressum'])\n",
    "site = 'http://www.liberale.de/page/pressemitteilungen'\n",
    "pages = ['{}?page=0%2C{}'.format(site, i) for i in range(1, 1063)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href'] and check(link['href'], blackwords)])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ParagraphExtractor(min_len=100)).save(path.join('data', 'Corpus_FDP_Fraktion.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.presseportal.de'\n",
    "keyword = '/pm/7846/'\n",
    "site = 'http://www.presseportal.de/nr/7846'\n",
    "pages = ['{}/{}'.format(site, i * 27) for i in range(1, 621)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ParagraphExtractor(min_len=150)).save(path.join('data', 'Corpus_CDU_Fraktion.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyword = '/?p='\n",
    "site = 'http://aktion-widerstand.de/?page_id=11042'\n",
    "pages = ['{}&paged={}'.format(site, i) for i in range(2, 335)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href']])\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ParagraphExtractor(min_len=100)).save(path.join('data', 'Corpus_NPD_Jung.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.npd-fraktion-mv.de'\n",
    "keyword = '&view=article&'\n",
    "blackwords = set(['content'])\n",
    "site = 'http://www.npd-fraktion-mv.de/index.php?com=news&view=archive'\n",
    "pages = ['{}&b={}&mid=8'.format(site, i * 50) for i in range(0, 38)]\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href'] and check(link['href'], blackwords)])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_NPD_MV.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.npd-fraktion-sachsen.de'\n",
    "blackwords = set(['meldungen', 'category', 'author'])\n",
    "site = 'http://www.npd-fraktion-sachsen.de/category/meldungen'\n",
    "pages = ['{}/page/{}'.format(site, i) for i in range(2, 194)]\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            blog = soup.find('div', id='blog-left')\n",
    "            links = set([link['href'] for link in blog.findAll('a')\n",
    "                        if check(link['href'], blackwords)])\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_NPD_Sachsen.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
