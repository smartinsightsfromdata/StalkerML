{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from requests import RequestException\n",
    "from os import path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from html2corpus import HTML2Corpus\n",
    "from html2corpus.extractors import ReadabilityExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling german political party sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Linke\n",
    "\n",
    "News from the website of the party: http://www.die-linke.de/nc/die-linke/nachrichten/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.die-linke.de'\n",
    "keyword = 'artikel'\n",
    "\n",
    "site = 'http://www.die-linke.de/nc/die-linke/nachrichten'\n",
    "pages = ['{}/browse/{}'.format(site, i) for i in range(1, 99)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_DieLinke.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press releases from the website of the party: http://www.die-linke.de/nc/presse/presseerklaerungen/presseerklaerungen/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.die-linke.de'\n",
    "keyword = 'artikel'\n",
    "site = 'http://www.die-linke.de/nc/presse/presseerklaerungen/presseerklaerungen'\n",
    "pages = ['{}/browse/{}'.format(site, i) for i in range(1, 272)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_DieLinke_PR.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPD\n",
    "\n",
    "Press releases from the website of the party: http://www.spdfraktion.de/presse/pressemitteilungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = 'http://www.spdfraktion.de'\n",
    "keyword = 'presse/pressemitteilungen/'\n",
    "blackword = 'feed'\n",
    "site = 'http://www.spdfraktion.de/presse/pressemitteilungen'\n",
    "pages = ['{}?page={}'.format(site, i) for i in range(1, 733)]\n",
    "pages.append(site)\n",
    "\n",
    "def get_data():\n",
    "    for page in pages:\n",
    "        try:\n",
    "            req = requests.get(page, timeout=10)\n",
    "            soup = BeautifulSoup(req.content)\n",
    "            links = set([link['href'] for link in soup.findAll('a')\n",
    "                     if keyword in link['href'] and blackword not in link['href']])\n",
    "            links = map(lambda x: '{}/{}'.format(domain, x), links)\n",
    "            for article in list(links):\n",
    "                article_req = requests.get(article)\n",
    "                yield article_req.content\n",
    "        except RequestException as error:\n",
    "            logging.error('Error: %s', error)\n",
    "            \n",
    "HTML2Corpus(get_data(), extractor=ReadabilityExtractor(min_len=100)).save(path.join('data', 'Corpus_SPD_PR.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
