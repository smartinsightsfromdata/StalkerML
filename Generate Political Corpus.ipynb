{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from os import path\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.externals import joblib\n",
    "from html2corpus import HTML2Corpus\n",
    "from html2corpus.extractors import ReadabilityExtractor, ParagraphExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = ['2015-44', '2015-45', '2015-46', '2015-47', '2015-48', '2015-49', '2015-50', '2015-51', \n",
    "          '2015-52', '2015-53', '2016-01', '2016-02', '2016-03', '2016-04']\n",
    "files = [path.join('data', 'CurrentNews', '{}.csv'.format(label)) for label in labels]\n",
    "dates = [('2015-10-26', '2015-11-02'),\n",
    "         ('2015-11-02', '2015-11-09'),\n",
    "         ('2015-11-09', '2015-11-16'),\n",
    "         ('2015-11-16', '2015-11-23'),\n",
    "         ('2015-11-23', '2015-11-30'),\n",
    "         ('2015-11-30', '2015-12-07'),\n",
    "         ('2015-12-07', '2015-12-14'),\n",
    "         ('2015-12-14', '2015-12-21'),\n",
    "         ('2015-12-21', '2015-12-28'),\n",
    "         ('2015-12-28', '2016-01-04'),\n",
    "         ('2016-01-04', '2016-01-11'),\n",
    "         ('2016-01-11', '2016-01-18'),\n",
    "         ('2016-01-18', '2016-01-25'),\n",
    "         ('2016-01-25', '2016-02-01')]\n",
    "\n",
    "db_file = path.join('..', 'Crawler', 'data', 'de_news.sqlite')\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = joblib.load(path.join('models', 'classifier', 'Vectorizer.pkl'))\n",
    "classifier = joblib.load(path.join('models', 'classifier', 'Classifier.pkl'))\n",
    "ext = ReadabilityExtractor(min_len=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the title has newlines and other shit in it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_html(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    for link in soup.find_all('a'):\n",
    "        link.extract()\n",
    "    for script in soup.find_all('script'):\n",
    "        script.extract()\n",
    "    text = soup.get_text().strip()\n",
    "    text = re.sub(r'[\\n\\t\\r\\[\\]]', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def classify(rows):\n",
    "    tfidf = vectorizer.transform(rows['text'])\n",
    "    return classifier.predict(tfidf)\n",
    "\n",
    "def get_data(kw):\n",
    "    query = '''SELECT title, summary, site, html \n",
    "               FROM Articles \n",
    "               LEFT JOIN Sources ON Articles.source = Sources.id \n",
    "               WHERE date BETWEEN ? and ?'''\n",
    "    cursor.execute(query, kw)\n",
    "\n",
    "    data = cursor.fetchmany(100)\n",
    "    \n",
    "    while len(data) > 0:\n",
    "        for row in data:\n",
    "            title, summary, site, html = row\n",
    "            text = ext.extract(html)\n",
    "            if len(text) > 0:\n",
    "                yield { \n",
    "                        'title': clean_html(title),\n",
    "                        'summary': clean_html(summary),\n",
    "                        'text': text, \n",
    "                        'site': site \n",
    "                }\n",
    "        data = cursor.fetchmany(100)\n",
    "        \n",
    "def process(file, date, i):\n",
    "    articles = pd.DataFrame([article for article in get_data(date)])\n",
    "    tags = classify(articles)\n",
    "    articles['week'] = labels[i]\n",
    "    articles['tag'] = tags\n",
    "    articles.loc[articles['tag'] == 1, ['week','title', 'summary', 'text', 'site']].to_csv(file, index=False, encoding='utf-8', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, (file, date) in enumerate(zip(files, dates)):\n",
    "    process(file, date, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
