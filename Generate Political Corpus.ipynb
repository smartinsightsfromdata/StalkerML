{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from os import path\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from html2corpus import HTML2Corpus\n",
    "from html2corpus.extractors import ReadabilityExtractor, ParagraphExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['2015KW44', '2015KW45', '2015KW46', '2015KW47', '2015KW48', '2015KW49', '2015KW50', '2015KW51']\n",
    "files = [path.join('data', 'CurrentNews', '{}.csv'.format(label)) for label in labels]\n",
    "dates = [('2015-10-26', '2015-11-02'),\n",
    "         ('2015-11-02', '2015-11-09'),\n",
    "         ('2015-11-09', '2015-11-16'),\n",
    "         ('2015-11-16', '2015-11-23'),\n",
    "         ('2015-11-23', '2015-11-30'),\n",
    "         ('2015-11-30', '2015-12-07'),\n",
    "         ('2015-12-07', '2015-12-14'),\n",
    "         ('2015-12-14', '2015-12-21')]\n",
    "\n",
    "db_file = path.join('..', 'Crawler', 'data', 'de_news.sqlite')\n",
    "conn = sqlite3.connect(db_file)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = joblib.load(path.join('models', 'classifier', 'Vectorizer.pkl'))\n",
    "classifier = joblib.load(path.join('models', 'classifier', 'Classifier.pkl'))\n",
    "ext = ReadabilityExtractor(min_len=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the title has newlines and other shit in it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(rows):\n",
    "    tfidf = vectorizer.transform(rows['text'])\n",
    "    return classifier.predict(tfidf)\n",
    "\n",
    "def get_data(kw):\n",
    "    query = '''SELECT site, html \n",
    "               FROM Articles \n",
    "               LEFT JOIN Sources ON Articles.source = Sources.id \n",
    "               WHERE date BETWEEN ? and ?'''\n",
    "    cursor.execute(query, kw)\n",
    "\n",
    "    data = cursor.fetchmany(100)\n",
    "    \n",
    "    while len(data) > 0:\n",
    "        for row in data:\n",
    "            site, html = row\n",
    "            text = ext.extract(html)\n",
    "            if len(text) > 0:\n",
    "                yield { 'text': text, 'site': site }\n",
    "        data = cursor.fetchmany(100)\n",
    "\n",
    "for file, date in zip(files, dates):\n",
    "    articles = pd.DataFrame([article for article in get_data(date)])\n",
    "    tags = classify(articles)\n",
    "    articles['tag'] = tags\n",
    "    articles.loc[articles['tag'] == 1].loc[:, ['text', 'site']].to_csv(file, index=False, encoding='utf-8', sep='|')\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
